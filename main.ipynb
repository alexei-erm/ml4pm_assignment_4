{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "from model import BaselineModel\n",
    "from dataloader import *\n",
    "from train_utils import train_baseline, train_coral, train_adversarial, train_adabn\n",
    "from plot_utils import *\n",
    "from analysis_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, device):\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.source_loader, self.target_loader = prepare_data(batch_size=args.batch_size)\n",
    "\n",
    "    def train_baseline(self):\n",
    "        model = BaselineModel().to(self.device)\n",
    "        return train_baseline(model, self.source_loader, self.target_loader, \n",
    "                            self.args, self.device)\n",
    "\n",
    "    def train_coral(self):\n",
    "        model = BaselineModel().to(self.device)\n",
    "        return train_coral(model, self.source_loader, self.target_loader, \n",
    "                          self.args, self.device)\n",
    "\n",
    "    def train_adversarial(self):\n",
    "        model = BaselineModel().to(self.device)\n",
    "        return train_adversarial(model, self.source_loader, self.target_loader, \n",
    "                               self.args, self.device)\n",
    "\n",
    "    def train_adabn(self):\n",
    "        model = BaselineModel().to(self.device)\n",
    "        return train_adabn(model, self.source_loader, self.target_loader, \n",
    "                          self.args, self.device)\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "    trainer = Trainer(args, device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if args.method == 'baseline' or args.method == 'all':\n",
    "        results['baseline'] = trainer.train_baseline()\n",
    "    \n",
    "    if args.method == 'coral' or args.method == 'all':\n",
    "        results['coral'] = trainer.train_coral()\n",
    "    \n",
    "    if args.method == 'adversarial' or args.method == 'all':\n",
    "        results['adversarial'] = trainer.train_adversarial()\n",
    "    \n",
    "    if args.method == 'adabn' or args.method == 'all':\n",
    "        results['adabn'] = trainer.train_adabn()\n",
    "    \n",
    "    print(\"\\nFinal Target Accuracies:\")\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"{model_name}: {model_results['final_target_acc']:.4f}\")\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    with open(f'results/results_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For notebook usage\n",
    "parser = argparse.ArgumentParser(description='Domain Adaptation Methods')\n",
    "parser.add_argument('--method', type=str, default='all',\n",
    "                    choices=['baseline', 'coral', 'adversarial', 'adabn', 'all'])\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--coral_weight', type=float, default=1.0)\n",
    "parser.add_argument('--adversarial_weight', type=float, default=1.0)\n",
    "args = parser.parse_args([]) \n",
    "\n",
    "results = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(results, save=True, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/hphmcz155530jnw34qpssk100000gn/T/ipykernel_8680/79721922.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexei.ermochkine/Desktop/ma5/ML4PM/venv4ml4pm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/alexei.ermochkine/Desktop/ma5/ML4PM/venv4ml4pm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/alexei.ermochkine/Desktop/ma5/ML4PM/venv4ml4pm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "baseline Summary:\n",
      "\n",
      "Source Domain Performance:\n",
      "Overall Accuracy: 1.0000\n",
      "\n",
      "Target Domain Performance:\n",
      "Overall Accuracy: 0.8000\n",
      "\n",
      "Per-class F1 scores (Source → Target):\n",
      "Fault 0: 1.0000 → 1.0000\n",
      "Fault 1: 1.0000 → 0.0000\n",
      "Fault 2: 1.0000 → 1.0000\n",
      "Fault 3: 1.0000 → 1.0000\n",
      "Fault 4: 1.0000 → 0.6667\n",
      "Fault 5: 1.0000 → 1.0000\n",
      "Fault 6: 1.0000 → 0.0000\n",
      "Fault 7: 1.0000 → 1.0000\n",
      "Fault 8: 1.0000 → 1.0000\n",
      "Fault 9: 1.0000 → 0.6667\n",
      "\n",
      "Analyzing coral...\n",
      "\n",
      "coral Summary:\n",
      "\n",
      "Source Domain Performance:\n",
      "Overall Accuracy: 1.0000\n",
      "\n",
      "Target Domain Performance:\n",
      "Overall Accuracy: 0.8950\n",
      "\n",
      "Per-class F1 scores (Source → Target):\n",
      "Fault 0: 1.0000 → 1.0000\n",
      "Fault 1: 1.0000 → 1.0000\n",
      "Fault 2: 1.0000 → 1.0000\n",
      "Fault 3: 1.0000 → 0.9474\n",
      "Fault 4: 1.0000 → 0.7843\n",
      "Fault 5: 1.0000 → 1.0000\n",
      "Fault 6: 1.0000 → 0.4615\n",
      "Fault 7: 1.0000 → 0.7917\n",
      "Fault 8: 1.0000 → 1.0000\n",
      "Fault 9: 1.0000 → 0.8649\n",
      "\n",
      "Analyzing adversarial...\n",
      "\n",
      "adversarial Summary:\n",
      "\n",
      "Source Domain Performance:\n",
      "Overall Accuracy: 1.0000\n",
      "\n",
      "Target Domain Performance:\n",
      "Overall Accuracy: 1.0000\n",
      "\n",
      "Per-class F1 scores (Source → Target):\n",
      "Fault 0: 1.0000 → 1.0000\n",
      "Fault 1: 1.0000 → 1.0000\n",
      "Fault 2: 1.0000 → 1.0000\n",
      "Fault 3: 1.0000 → 1.0000\n",
      "Fault 4: 1.0000 → 1.0000\n",
      "Fault 5: 1.0000 → 1.0000\n",
      "Fault 6: 1.0000 → 1.0000\n",
      "Fault 7: 1.0000 → 1.0000\n",
      "Fault 8: 1.0000 → 1.0000\n",
      "Fault 9: 1.0000 → 1.0000\n",
      "\n",
      "Analyzing adabn...\n",
      "\n",
      "adabn Summary:\n",
      "\n",
      "Source Domain Performance:\n",
      "Overall Accuracy: 1.0000\n",
      "\n",
      "Target Domain Performance:\n",
      "Overall Accuracy: 0.9700\n",
      "\n",
      "Per-class F1 scores (Source → Target):\n",
      "Fault 0: 1.0000 → 1.0000\n",
      "Fault 1: 1.0000 → 0.9474\n",
      "Fault 2: 1.0000 → 1.0000\n",
      "Fault 3: 1.0000 → 1.0000\n",
      "Fault 4: 1.0000 → 0.9302\n",
      "Fault 5: 1.0000 → 1.0000\n",
      "Fault 6: 1.0000 → 0.9189\n",
      "Fault 7: 1.0000 → 0.9744\n",
      "Fault 8: 1.0000 → 1.0000\n",
      "Fault 9: 1.0000 → 0.9302\n",
      "\n",
      "baseline - Classes most affected by domain shift:\n",
      "Fault 1: F1-score drop of 1.0000\n",
      "Fault 6: F1-score drop of 1.0000\n",
      "Fault 4: F1-score drop of 0.3333\n",
      "Fault 9: F1-score drop of 0.3333\n",
      "Fault 0: F1-score drop of 0.0000\n",
      "Fault 2: F1-score drop of 0.0000\n",
      "Fault 3: F1-score drop of 0.0000\n",
      "Fault 5: F1-score drop of 0.0000\n",
      "Fault 7: F1-score drop of 0.0000\n",
      "Fault 8: F1-score drop of 0.0000\n",
      "\n",
      "coral - Classes most affected by domain shift:\n",
      "Fault 6: F1-score drop of 0.5385\n",
      "Fault 4: F1-score drop of 0.2157\n",
      "Fault 7: F1-score drop of 0.2083\n",
      "Fault 9: F1-score drop of 0.1351\n",
      "Fault 3: F1-score drop of 0.0526\n",
      "Fault 0: F1-score drop of 0.0000\n",
      "Fault 1: F1-score drop of 0.0000\n",
      "Fault 2: F1-score drop of 0.0000\n",
      "Fault 5: F1-score drop of 0.0000\n",
      "Fault 8: F1-score drop of 0.0000\n",
      "\n",
      "adversarial - Classes most affected by domain shift:\n",
      "Fault 0: F1-score drop of 0.0000\n",
      "Fault 1: F1-score drop of 0.0000\n",
      "Fault 2: F1-score drop of 0.0000\n",
      "Fault 3: F1-score drop of 0.0000\n",
      "Fault 4: F1-score drop of 0.0000\n",
      "Fault 5: F1-score drop of 0.0000\n",
      "Fault 6: F1-score drop of 0.0000\n",
      "Fault 7: F1-score drop of 0.0000\n",
      "Fault 8: F1-score drop of 0.0000\n",
      "Fault 9: F1-score drop of 0.0000\n",
      "\n",
      "adabn - Classes most affected by domain shift:\n",
      "Fault 6: F1-score drop of 0.0811\n",
      "Fault 4: F1-score drop of 0.0698\n",
      "Fault 9: F1-score drop of 0.0698\n",
      "Fault 1: F1-score drop of 0.0526\n",
      "Fault 7: F1-score drop of 0.0256\n",
      "Fault 0: F1-score drop of 0.0000\n",
      "Fault 2: F1-score drop of 0.0000\n",
      "Fault 3: F1-score drop of 0.0000\n",
      "Fault 5: F1-score drop of 0.0000\n",
      "Fault 8: F1-score drop of 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Create fresh model instances and load the saved states\n",
    "def load_model_with_states(path, device=\"cpu\"):\n",
    "    model = BaselineModel().to(device)\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if isinstance(state_dict, dict) and 'model' in state_dict:\n",
    "        # For adversarial model that includes discriminator\n",
    "        model.load_state_dict(state_dict['model'])\n",
    "    else:\n",
    "        # For other models\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create models dictionary with proper model instances\n",
    "models_dict = {\n",
    "    'baseline': load_model_with_states(\"models/baseline_final.pth\", device=\"cpu\"),\n",
    "    'coral': load_model_with_states(\"models/coral_final.pth\", device=\"cpu\"),\n",
    "    'adversarial': load_model_with_states(\"models/adversarial_final.pth\", device=\"cpu\"),\n",
    "    'adabn': load_model_with_states(\"models/adabn_final.pth\", device=\"cpu\")\n",
    "}\n",
    "\n",
    "# Optional: Define your class names (fault types)\n",
    "class_names = [f'Fault {i}' for i in range(10)]  # Replace with actual fault names if available\n",
    "\n",
    "trainer = Trainer(args, device=\"cpu\")\n",
    "\n",
    "# Run the analysis\n",
    "performance_results = analyze_per_class_performance(\n",
    "    models_dict,\n",
    "    trainer.source_loader,\n",
    "    trainer.target_loader,\n",
    "    device=\"cpu\",\n",
    "    class_names=class_names,\n",
    "    save_plots=True\n",
    ")\n",
    "\n",
    "# Print domain shift analysis\n",
    "print_domain_shift_analysis(performance_results, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing feature space for baseline...\n",
      "\n",
      "baseline Domain Statistics:\n",
      "Mean Distance: 2.0196\n",
      "Covariance Distance: 15.1903\n",
      "MMD Distance: 4.0787\n",
      "\n",
      "Analyzing feature space for coral...\n",
      "\n",
      "coral Domain Statistics:\n",
      "Mean Distance: 0.0641\n",
      "Covariance Distance: 0.0401\n",
      "MMD Distance: 0.0041\n",
      "\n",
      "Analyzing feature space for adversarial...\n",
      "\n",
      "adversarial Domain Statistics:\n",
      "Mean Distance: 1.2607\n",
      "Covariance Distance: 10.5836\n",
      "MMD Distance: 1.5894\n",
      "\n",
      "Analyzing feature space for adabn...\n",
      "\n",
      "adabn Domain Statistics:\n",
      "Mean Distance: 1.6048\n",
      "Covariance Distance: 13.6504\n",
      "MMD Distance: 2.5753\n"
     ]
    }
   ],
   "source": [
    "# Run the feature space analysis\n",
    "feature_analysis_results = analyze_feature_space(\n",
    "    models_dict,\n",
    "    trainer.source_loader,\n",
    "    trainer.target_loader,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# The analysis will create several visualizations in the plots/feature_space/ directory:\n",
    "# 1. t-SNE visualizations for each model\n",
    "# 2. PCA visualizations for each model\n",
    "# 3. Comparative bar plots of domain distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv4ml4pm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
